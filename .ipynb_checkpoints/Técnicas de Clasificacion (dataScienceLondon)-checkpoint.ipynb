{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Técnicas de clasificación\n",
    "\n",
    "Para valorar el rendimiento de cada técnica, se ha realizado un estudio con el dataset de Londres proporcionado por Kaggle\n",
    "\n",
    "https://www.kaggle.com/c/data-science-london-scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M:\\\\Users\\\\angui\\\\Desktop\\\\proyectos_personales\\\\Trabajos_Kaggle'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./LondonDataSet/train.csv', header = None)\n",
    "trainLabels = pd.read_csv('./LondonDataSet/trainLabels.csv', header = None)\n",
    "test = pd.read_csv('./LondonDataSet/test.csv', header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "En este caso, el preprocesamiento consiste en dividir en train y test debido a que no hay que hacer ninguna depuración de datos ni extracción de nuevos atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train, trainLabels, test_size = 0.3, random_state = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Gaussian Naive - Bayes\n",
    "\n",
    "Conjunto de algoritmos supervisados basados en aplicar el teorema de bayes asumiendo \"ingenuidad\".\n",
    "\n",
    "## Ventajas\n",
    "\n",
    "- Fácil y rápido prediciendo la clase.\n",
    "\n",
    "- Funciona bien en la predicción multiclase\n",
    "    \n",
    "- Si se asumen independencia, funciona mucho mejor que otros como la Regresión Logística.\n",
    "    \n",
    "- Funcionan mejor con variables de entrada categóricas que numéricas\n",
    "    \n",
    "## Inconvenientes\n",
    "  \n",
    "- Si aparece una categoría en el conjunto de datos que pueba que no estaba en el conjunto de entrenamiento, se asignara una probabilidad 0 y no se podrá hacer una predicción. Esto se conoce como frecuencia 0 y para resolverlo podemos usar la \"técnica de aislamiento\".\n",
    "\n",
    "- Es muy difícil que en la vida real podamos asumir independencia completa.\n",
    "\n",
    "## Ejemplos de uso:\n",
    "\n",
    "- Para saber si un e-mail es o no spam.\n",
    "\n",
    "- Para clasificar artículos en función de si es tecnológico, político o deportivo.\n",
    "    \n",
    "- Para saber si una parte de un texto expresa emociones positivas o negativas.\n",
    "    \n",
    "- Reconocimiento facial.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Para mas información consultar https://scikit-learn.org/stable/modules/naive_bayes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes :  0.8033333333333333\n"
     ]
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(x_train, y_train.values.ravel())\n",
    "predicted = model.predict(x_test)\n",
    "GaussNB = accuracy_score(y_test, predicted)\n",
    "print('Naive Bayes : ', GaussNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. KNN (K- nearest neighbor) (K- vecino más cercano)\n",
    "\n",
    "El método del vecino más cercano consiste en buscar un número predefinido de muestras de entrenamiento lo suficientemente cerca con un nuevo punto, y predecir el atributo a partir de ahí.\n",
    "\n",
    "## Ventajas\n",
    "\n",
    "- No paramétrico\n",
    "\n",
    "- Fácil de  explicar, comprender e interpretar\n",
    "\n",
    "- Alta precisión (no competitiva en comparación con otros  modelos)\n",
    "\n",
    "- Insensible a valores atípicos\n",
    "\n",
    "## Inconvenientes\n",
    "\n",
    "- No aprende como tal, sino que memoriza instancias que luego usa como conocimiento en la fase de predicción.\n",
    "\n",
    "- Computacionalmente costoso debido a que almacena en memorio todo el dataset de entrenamiento.\n",
    "\n",
    "\n",
    "## Ejemplos de uso:\n",
    "\n",
    "- Prediccion de dígitos escritos a mano.\n",
    " \n",
    "- Imágenes de satélites\n",
    "\n",
    "- Se suele usar cuando las clasificaciones que hacemos son muy irregulares...\n",
    "\n",
    "\n",
    "\n",
    "Para más información consultar https://scikit-learn.org/stable/modules/neighbors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN :  0.8733333333333333\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier()\n",
    "model.fit(x_train, y_train.values.ravel())\n",
    "predicted = model.predict(x_test)\n",
    "knn = accuracy_score(y_test, predicted)\n",
    "print('KNN : ', knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Forest\n",
    "\n",
    "El Random Forest es una técnica de aprendizaje automático llamada árbol de decisiones debido a que se introduce una entreada en la parte superior, y a medida que atraviese el árbol, los datos se acumulan en conjuntos más pequeños.\n",
    "\n",
    "\n",
    "\n",
    "## Ventajas\n",
    "\n",
    "- Como  existen muy pocas suposiciones, la preparación de los datos es mínima.\n",
    "\n",
    "- Puede manejar miles de variables de entreada e iddentificar las más significativas (reducción de dimensionalidad).\n",
    "\n",
    "- Una de las salidas del modelos es la importancia de las vaairables\n",
    "\n",
    "- Métodos efectivos para valores missing\n",
    "\n",
    "- Es posible usarlo como \"clustering\"\n",
    "\n",
    "- Es eficiente en grandes bases de datos\n",
    "\n",
    "- Tiene métodos para error de equilibrio en la población de conjuntos de datos no balanceados.\n",
    "\n",
    "\n",
    "## Incovenientes\n",
    "\n",
    "- Pérdida de interpretación (muy dificil interpretar los cientos de arboles creados en el bosque)\n",
    "\n",
    "- Bueno  para clasificación , no para regresión.\n",
    "\n",
    "- Poco control de lo que hace el modelo (modelo de caja negra)\n",
    "\n",
    "- No funciona con datasets pequeños\n",
    "\n",
    "## Ejemplos de uso:\n",
    "\n",
    "- Detección de fraude en medios de pago\n",
    "\n",
    "- Emisiones de autobuses urbanos\n",
    "\n",
    "- Análisis de mercados\n",
    "\n",
    "http://randomforest2013.blogspot.com/2013/05/randomforest-definicion-random-forests.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=random%20forest#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest :  0.85\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators = 100, random_state = 99)\n",
    "model.fit(x_train,y_train.values.ravel())\n",
    "predicted = model.predict(x_test)\n",
    "Rf = accuracy_score(y_test, predicted)\n",
    "print('Random Forest : ',Rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Regresión Logística\n",
    "\n",
    "Sirve normalmente para moddelar la probabilidad de un evento como función de otros factores.\n",
    "\n",
    "\n",
    "## Ventajas\n",
    "\n",
    "- Eficaz y simple\n",
    "\n",
    "- Resultados altamente interpretables\n",
    "\n",
    "- El peso de las características determina la importancia en la decisión final.\n",
    "\n",
    "- Funciona mejor cuando se usan atributos relacionados con el de salida.\n",
    "\n",
    "- Es clave hacer las seleccion de características previas al entrenamiento, debido a que es importante eliminar las características con una gran multicolinealidad entre sí.\n",
    "\n",
    "## Inconvenientes\n",
    "\n",
    "- Imposibilidad de resolver problemas no lineales\n",
    "\n",
    "- La variable objetivo debe ser linealmente separable (dos regiones de datos)\n",
    "\n",
    "- No es una herramienta útil para identificar las características más adecuadas\n",
    "\n",
    "- Existen otros algoritmos más potentes que éste\n",
    "\n",
    "## Ejemplos de uso:\n",
    "\n",
    "- Investigaación en endocrinología (en concreto en pacientes con diabetes)\n",
    "\n",
    "- Predecir el riesgo de sufrir una enferrmedad coronária\n",
    "\n",
    "- Asociacion de dependencia de personas mayores a 65 años\n",
    "\n",
    "- Si el origen familiar influye en la inserción laboral\n",
    "\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression :  0.81\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver = 'saga')\n",
    "model.fit(x_train,y_train.values.ravel())\n",
    "predicted = model.predict(x_test)\n",
    "Lr = accuracy_score(y_test, predicted)\n",
    "print('Logistic Regression : ',Lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Support Vector Machine (SVM) (Máquina de vector soporte)\n",
    "\n",
    "Las máquinas de vector soport se basan en contruir un hiperplano (donde cada característica es una dimensión) que separe las clases lo máximo posible en función de los hiperparámetros.\n",
    "\n",
    "## Ventajas\n",
    "\n",
    "- Más rápido que Naive-Bayes\n",
    "\n",
    "- Funciona bien con un espacio dimensional elevado\n",
    "\n",
    "- Es efectivo cuando el número de  dimensiones es superior al número de registros\n",
    "\n",
    "- Muy eficiente en cuanto a memoria\n",
    "\n",
    "\n",
    "## Inconvenientes\n",
    "\n",
    "- No funciona bien si el dataset tiene mucho ruido o cuando las clases se superponen\n",
    "\n",
    "- Es sensible al tipo de núcleo usado\n",
    "\n",
    "- No es adecuada para grandes conjuntos de datos\n",
    "\n",
    "- No hay una explicación probabilística para la clasificación\n",
    "\n",
    "\n",
    "## Ejemplos de uso:\n",
    "\n",
    "- Reconocimiento facial\n",
    "\n",
    "- Clasificación de imágenes\n",
    "\n",
    "- RReconocimiento de escritura a mano\n",
    "\n",
    "- Detección de homología (mismas características) entre proteínas\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM :  0.89\n"
     ]
    }
   ],
   "source": [
    "model = SVC(gamma = 'auto')\n",
    "model.fit(x_train,y_train.values.ravel())\n",
    "predicted = model.predict(x_test)\n",
    "Svm = accuracy_score(y_test, predicted)\n",
    "print('SVM : ',Svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Árbol de decisión\n",
    "\n",
    "Como el propio nombre indica, es un arbol que se basa en predecir una variable mediante decisiones simples que va aprendiendo a lo largo del modelo.\n",
    "\n",
    "## Ventajas\n",
    "- Fácil de usar e interpretar. Se podría llamar un modelo de \"caja blanca\" porque todas las decisiones se explican.\n",
    "\n",
    "- Predice tanto variables numéricas como categóricas\n",
    "\n",
    "- No requiere mucho preprocesamiento\n",
    "\n",
    "- Es un modelo no paramétrico (no asume ninguna dimension de los datos)\n",
    "\n",
    "- Las características menos importantes no influencian en el resultado.\n",
    "\n",
    "- Que exista multicolinealidad entre características no afecta a la calidad.\n",
    "\n",
    "## Inconvenientes\n",
    "\n",
    "- Tienden a hacer sobreentrenamiento.\n",
    "\n",
    "- Un pequeño cambio en los datos puede hacer que cambie la estructura completa del árbol\n",
    "\n",
    "- Los árboles suelen necesitar tiempos altos de entrenamiento.\n",
    "\n",
    "- Son muy caros en cuanto a complejidad y tiempo.\n",
    "\n",
    "- No se debe usar para aplicar regresión ni variables numéricas continuas.\n",
    "\n",
    "## Ejemplos de uso\n",
    "\n",
    "- Crear inteligencias para juegos de estrategia.\n",
    "\n",
    "- Cualquier cosa cotidiana de decisión.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/tree.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arbol de decisión :  0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_train,y_train.values.ravel())\n",
    "predicted = model.predict(x_test)\n",
    "d_tree = accuracy_score(y_test, predicted)\n",
    "print('Arbol de decisión : ',d_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. XGBoost\n",
    "Librería que optimiza el método del descenso del gradiente combinado con el método boosting para ser más eficiente y flexible. \n",
    "Es una técnica tanto de clasificación como de regresión que produce un modelo predictivo usando varios árboles de decisión.\n",
    "\n",
    "## Ventajas\n",
    "\n",
    "- Funciona muy bien con todo tipo de datos menos con datos muy dispersos\n",
    "- Tiene una naturaleza tipo \"caja negra\"\n",
    "- Ventajas parecidas a los árboles de decisión debido a que trabajo con ellos.\n",
    "- Funciona mejor que la mayoría de algoritmos de aprendizaje supervisado\n",
    "- Funciona tanto para predecir variables numéricas, como categóricas\n",
    "\n",
    "## Inconvenientes\n",
    "\n",
    "- Inconvenientes parecidos a los árboles de decisión debido a que trabajo con ellos.\n",
    "- No suele haber aplicaciones en el mundo real de datos estructurados\n",
    "\n",
    "## Ejemplos de uso\n",
    "\n",
    "- Analisis de supervivencia\n",
    "- Clasificar productos en distintas categorías\n",
    "- Prácticamente sirve para todo\n",
    "\n",
    "https://xgboost.readthedocs.io/en/latest/get_started.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost :  0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(x_train,y_train.values.ravel())\n",
    "predicted = model.predict(x_test)\n",
    "XGB = accuracy_score(y_test, predicted)\n",
    "print('XGBoost : ', XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen de la calidad de los 7 primeros modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Naive-Bayes :  0.8033333333333333\n",
      "-------------------------------\n",
      "KNN :  0.8733333333333333\n",
      "-------------------------------\n",
      "Random Forest :  0.85\n",
      "-------------------------------\n",
      "Logistic Regression :  0.81\n",
      "-------------------------------\n",
      "SVM :  0.89\n",
      "-------------------------------\n",
      "Arbol de decisión :  0.7333333333333333\n",
      "-------------------------------\n",
      "XGBoost :  0.8666666666666667\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-------------------------------')\n",
    "print('Naive-Bayes : ', GaussNB)\n",
    "print('-------------------------------')\n",
    "print('KNN : ', knn)\n",
    "print('-------------------------------')\n",
    "print('Random Forest : ',Rf)\n",
    "print('-------------------------------')\n",
    "print('Logistic Regression : ',Lr)\n",
    "print('-------------------------------')\n",
    "print('SVM : ',Svm)\n",
    "print('-------------------------------')\n",
    "print('Arbol de decisión : ',d_tree)\n",
    "print('-------------------------------')\n",
    "print('XGBoost : ', XGB)\n",
    "print('-------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Adaboost\n",
    "\n",
    "Estimador que consiste en adecuar un clasificador al dataset original, y después adecuarlo a copias del clasificador en el mismo dataset pero con los pesos ajustados de las clases mal clasificadas.\n",
    "\n",
    "## Ventajas\n",
    "- Es menos necesario ajustar parámetros (ej: SVM)\n",
    "- Teóricamente, el algoritmo AdaBoost no hace overfit (aunque no hay pruebas de ello)\n",
    "- Fácil de programar\n",
    "- Son muy versátiles tanto para datos numéricos como para datos de texto\n",
    "\n",
    "\n",
    "## Inconvenientes\n",
    "\n",
    "- Las técnicas de boosting tardan en entrenar\n",
    "- Es muy sensible al ruido y a los outliers\n",
    "- El algoritmo XGBoost es más rápido\n",
    "\n",
    "## Ejemplos de uso\n",
    "\n",
    "- Detección facial\n",
    "- Reconocimiento de imágenes\n",
    "- Admision o no de un estudiante a una escuela\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0cb0e039b307>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mAdaB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'AdaBoost : '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAdaB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier(n_estimators = 40)\n",
    "model.fit(x_train,y_train.values.ravel())\n",
    "predicted = model.predict(x_test)\n",
    "AdaB = accuracy_score(y_test, predicted)\n",
    "print('AdaBoost : ', AdaB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "funciones = ['1. Gaussian-Naive-Bayes','2. KNN (vecino más cercano)', '3. Random Forest', '4. Regresión Logística', \n",
    "             '5. Maquina de vector soporte', '6. Árbol de decisión', '7. XGBoost', '8. AdaBoost' ]\n",
    "nombre_funciones = ['GaussianNB()', 'KNeighborsClassifier()','RandomForestClassifier()',\n",
    "                    'LogisticRegression()','SVC()', 'DecisionTreeClassifier()','XGBClassifier()', 'AdaBoostClassifier()']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Gaussian-Naive-Bayes', '2. KNN (vecino más cercano)', '3. Random Forest', '4. Regresión Logística', '5. Maquina de vector soporte', '6. Árbol de decisión', '7. XGBoost', '8. AdaBoost']\n"
     ]
    }
   ],
   "source": [
    "print(funciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcion_a_escoger_e_hiperparametros():\n",
    "    print('Las siguientes funciones son las que están disponibles en este momento :')\n",
    "    for funcion in funciones:\n",
    "        print(funcion)\n",
    "    \n",
    "    respuesta = int(input('Escriba el número de la función que desea utilizar :'))\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        nombre_algoritmo = nombre_funciones[respuesta-1]\n",
    "        print('Has escogido el algoritmo ' + str(funciones[respuesta - 1]))\n",
    "        print('El algoritmo se llama ' + str(nombre_algoritmo))\n",
    "    except:\n",
    "        print('--------ERROR : El número a escoger debe estar entre 1 y ' + str(len(funciones))+ '----------')\n",
    "    \n",
    "    modelo = eval(nombre_algoritmo)\n",
    "    print('Este algoritmo tiene los siguientes parámetros: ')\n",
    "    \n",
    "    params = modelo.get_params()\n",
    "    for key in params : \n",
    "        print('\\t' +  key, ':' , params[key])\n",
    "    \n",
    "    diccionario = {}\n",
    "    \n",
    "    x = int(input('¿Quieres modificar algún parámetro?' + '\\n' +  '0.No - 1.Si: '))\n",
    "    \n",
    "    while x == 1:\n",
    "        respuesta2 = input('Elige el parámetro a modificar :' + '\\n'\n",
    "                       '(El nombre correspondiente al nombre del parámetro en la librería)')\n",
    "    \n",
    "        print('El parámetro : ' + str(respuesta2) + ' tiene ahora mismo el valor ' + str(params[respuesta2]))\n",
    "        \n",
    "        respuesta3 = input('¿Cuál quieres que sea su nuevo valor?: ')\n",
    "        \n",
    "        try:\n",
    "            respuesta3  = eval(respuesta3)\n",
    "        except:\n",
    "            print('Warning : el tipo será string' )\n",
    "        #print(type(a))\n",
    "        diccionario[respuesta2] = respuesta3\n",
    "        x = int(input('¿Quieres modificar algún parámetro más?' + '\\n' +  '0.No - 1.Si: '))\n",
    "        \n",
    "    #try :\n",
    "    for i in range(len(diccionario)):\n",
    "        modelo.set_params(**diccionario)\n",
    "    print(modelo)\n",
    "    #except:\n",
    "    #    print('Existe algún parámetro que no se ha escrito bien')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las siguientes funciones son las que están disponibles en este momento :\n",
      "1. Gaussian-Naive-Bayes\n",
      "2. KNN (vecino más cercano)\n",
      "3. Random Forest\n",
      "4. Regresión Logística\n",
      "5. Maquina de vector soporte\n",
      "6. Árbol de decisión\n",
      "7. XGBoost\n",
      "8. AdaBoost\n",
      "Escriba el número de la función que desea utilizar :2\n",
      "\n",
      "Has escogido el algoritmo 2. KNN (vecino más cercano)\n",
      "El algoritmo se llama KNeighborsClassifier()\n",
      "Este algoritmo tiene los siguientes parámetros: \n",
      "\talgorithm : auto\n",
      "\tleaf_size : 30\n",
      "\tmetric : minkowski\n",
      "\tmetric_params : None\n",
      "\tn_jobs : None\n",
      "\tn_neighbors : 5\n",
      "\tp : 2\n",
      "\tweights : uniform\n",
      "¿Quieres modificar algún parámetro?\n",
      "0.No - 1.Si: 0\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "funcion_a_escoger_e_hiperparametros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'SAMME.R', 'base_estimator': None, 'learning_rate': 1.0, 'n_estimators': 50, 'random_state': None, 'n_estimator': 'a'}\n",
      "{'algorithm': 'SAMME.R', 'base_estimator': None, 'learning_rate': 1.0, 'n_estimators': 50, 'random_state': None}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "var = model.get_params()\n",
    "var['n_estimator'] = 'a'\n",
    "print(var)\n",
    "print(model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm : SAMME.R\n",
      "base_estimator : None\n",
      "learning_rate : 1.0\n",
      "n_estimators : 50\n",
      "random_state : None\n"
     ]
    }
   ],
   "source": [
    "for key in var:\n",
    "    print(key, ':' , var[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(var['random_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter someparam for estimator AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n                   n_estimators=50, random_state=None). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-b9fb3c7c850a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msomeparam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'n_estimators_150'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mM:\\Anaconda\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mset_params\u001b[1;34m(self, **params)\u001b[0m\n\u001b[0;32m    222\u001b[0m                                  \u001b[1;34m'Check the list of available parameters '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m                                  \u001b[1;34m'with `estimator.get_params().keys()`.'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m                                  (key, self))\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdelim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter someparam for estimator AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n                   n_estimators=50, random_state=None). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "model.set_params(someparam = 'n_estimators_150')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= 0\n",
    "while x == 0:\n",
    "    a = input('elige un tipo')\n",
    "    try:\n",
    "        a  = eval(a)\n",
    "    except:\n",
    "        print('Warning : el tipo será string' )\n",
    "    print(type(a))\n",
    "    x = int(input('Seguimos? 0.Si - 1.No : '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = 'AdaBoostClassifier()'\n",
    "aux2 = eval(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                   n_estimators=60, random_state=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux2.set_params(n_estimators = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 50, 'learning_rate': 1.2}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictp = {}\n",
    "dictp['n_estimators'] = 50\n",
    "dictp['learning_rate'] = 1.2\n",
    "dictp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.2,\n",
       "                   n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux2.set_params(**dictp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
