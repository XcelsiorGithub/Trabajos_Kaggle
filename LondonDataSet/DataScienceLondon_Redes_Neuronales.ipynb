{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras import utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow.keras.layers import Activation, Input, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Sequential\n",
    "import scipy as sc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('M:/Users/angui/Desktop/proyectos_personales/dataScienceLondon/train.csv', header = None)\n",
    "trainLabels = pd.read_csv('M:/Users/angui/Desktop/proyectos_personales/dataScienceLondon/trainLabels.csv', header = None)\n",
    "test = pd.read_csv('M:/Users/angui/Desktop/proyectos_personales/dataScienceLondon/test.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.299403</td>\n",
       "      <td>-1.226624</td>\n",
       "      <td>1.498425</td>\n",
       "      <td>-1.176150</td>\n",
       "      <td>5.289853</td>\n",
       "      <td>0.208297</td>\n",
       "      <td>2.404498</td>\n",
       "      <td>1.594506</td>\n",
       "      <td>-0.051608</td>\n",
       "      <td>0.663234</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.850465</td>\n",
       "      <td>-0.622990</td>\n",
       "      <td>-1.833057</td>\n",
       "      <td>0.293024</td>\n",
       "      <td>3.552681</td>\n",
       "      <td>0.717611</td>\n",
       "      <td>3.305972</td>\n",
       "      <td>-2.715559</td>\n",
       "      <td>-2.682409</td>\n",
       "      <td>0.101050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-1.174176</td>\n",
       "      <td>0.332157</td>\n",
       "      <td>0.949919</td>\n",
       "      <td>-1.285328</td>\n",
       "      <td>2.199061</td>\n",
       "      <td>-0.151268</td>\n",
       "      <td>-0.427039</td>\n",
       "      <td>2.619246</td>\n",
       "      <td>-0.765884</td>\n",
       "      <td>-0.093780</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.819750</td>\n",
       "      <td>0.012037</td>\n",
       "      <td>2.038836</td>\n",
       "      <td>0.468579</td>\n",
       "      <td>-0.517657</td>\n",
       "      <td>0.422326</td>\n",
       "      <td>0.803699</td>\n",
       "      <td>1.213219</td>\n",
       "      <td>1.382932</td>\n",
       "      <td>-1.817761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.192222</td>\n",
       "      <td>-0.414371</td>\n",
       "      <td>0.067054</td>\n",
       "      <td>-2.233568</td>\n",
       "      <td>3.658881</td>\n",
       "      <td>0.089007</td>\n",
       "      <td>0.203439</td>\n",
       "      <td>-4.219054</td>\n",
       "      <td>-1.184919</td>\n",
       "      <td>-1.240310</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.604501</td>\n",
       "      <td>0.750054</td>\n",
       "      <td>-3.360521</td>\n",
       "      <td>0.856988</td>\n",
       "      <td>-2.751451</td>\n",
       "      <td>-1.582735</td>\n",
       "      <td>1.672246</td>\n",
       "      <td>0.656438</td>\n",
       "      <td>-0.932473</td>\n",
       "      <td>2.987436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.573270</td>\n",
       "      <td>-0.580318</td>\n",
       "      <td>-0.866332</td>\n",
       "      <td>-0.603812</td>\n",
       "      <td>3.125716</td>\n",
       "      <td>0.870321</td>\n",
       "      <td>-0.161992</td>\n",
       "      <td>4.499666</td>\n",
       "      <td>1.038741</td>\n",
       "      <td>-1.092716</td>\n",
       "      <td>...</td>\n",
       "      <td>1.022959</td>\n",
       "      <td>1.275598</td>\n",
       "      <td>-3.480110</td>\n",
       "      <td>-1.065252</td>\n",
       "      <td>2.153133</td>\n",
       "      <td>1.563539</td>\n",
       "      <td>2.767117</td>\n",
       "      <td>0.215748</td>\n",
       "      <td>0.619645</td>\n",
       "      <td>1.883397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.613071</td>\n",
       "      <td>-0.644204</td>\n",
       "      <td>1.112558</td>\n",
       "      <td>-0.032397</td>\n",
       "      <td>3.490142</td>\n",
       "      <td>-0.011935</td>\n",
       "      <td>1.443521</td>\n",
       "      <td>-4.290282</td>\n",
       "      <td>-1.761308</td>\n",
       "      <td>0.807652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513906</td>\n",
       "      <td>-1.803473</td>\n",
       "      <td>0.518579</td>\n",
       "      <td>-0.205029</td>\n",
       "      <td>-4.744566</td>\n",
       "      <td>-1.520015</td>\n",
       "      <td>1.830651</td>\n",
       "      <td>0.870772</td>\n",
       "      <td>-1.894609</td>\n",
       "      <td>0.408332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>-0.310429</td>\n",
       "      <td>0.826811</td>\n",
       "      <td>-0.952245</td>\n",
       "      <td>0.768850</td>\n",
       "      <td>1.877520</td>\n",
       "      <td>1.320646</td>\n",
       "      <td>1.944609</td>\n",
       "      <td>1.191420</td>\n",
       "      <td>-0.127724</td>\n",
       "      <td>0.070937</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.600411</td>\n",
       "      <td>-0.383792</td>\n",
       "      <td>0.745596</td>\n",
       "      <td>-0.698598</td>\n",
       "      <td>-2.729937</td>\n",
       "      <td>-0.431535</td>\n",
       "      <td>0.372873</td>\n",
       "      <td>1.019092</td>\n",
       "      <td>-2.672811</td>\n",
       "      <td>-0.295141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>-1.853879</td>\n",
       "      <td>0.246726</td>\n",
       "      <td>0.459921</td>\n",
       "      <td>-2.074267</td>\n",
       "      <td>7.599220</td>\n",
       "      <td>-0.138355</td>\n",
       "      <td>-4.501900</td>\n",
       "      <td>0.630634</td>\n",
       "      <td>-1.590533</td>\n",
       "      <td>-1.112949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361736</td>\n",
       "      <td>0.240052</td>\n",
       "      <td>-0.856196</td>\n",
       "      <td>-0.072481</td>\n",
       "      <td>-2.935896</td>\n",
       "      <td>0.582411</td>\n",
       "      <td>-2.613407</td>\n",
       "      <td>0.036687</td>\n",
       "      <td>2.809310</td>\n",
       "      <td>4.412567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>0.912748</td>\n",
       "      <td>-1.734039</td>\n",
       "      <td>-1.047035</td>\n",
       "      <td>0.217573</td>\n",
       "      <td>13.457812</td>\n",
       "      <td>0.162771</td>\n",
       "      <td>-2.250521</td>\n",
       "      <td>2.216161</td>\n",
       "      <td>-0.378326</td>\n",
       "      <td>0.642114</td>\n",
       "      <td>...</td>\n",
       "      <td>1.195896</td>\n",
       "      <td>-1.073806</td>\n",
       "      <td>-2.754369</td>\n",
       "      <td>1.814864</td>\n",
       "      <td>-4.190105</td>\n",
       "      <td>-1.116441</td>\n",
       "      <td>-2.100125</td>\n",
       "      <td>0.061513</td>\n",
       "      <td>0.895536</td>\n",
       "      <td>0.813686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>2.439780</td>\n",
       "      <td>-0.735511</td>\n",
       "      <td>-0.902426</td>\n",
       "      <td>1.365036</td>\n",
       "      <td>-10.430299</td>\n",
       "      <td>-0.856859</td>\n",
       "      <td>2.686474</td>\n",
       "      <td>0.292035</td>\n",
       "      <td>0.585388</td>\n",
       "      <td>-0.876965</td>\n",
       "      <td>...</td>\n",
       "      <td>2.262326</td>\n",
       "      <td>-0.039488</td>\n",
       "      <td>0.773876</td>\n",
       "      <td>-0.916066</td>\n",
       "      <td>2.604827</td>\n",
       "      <td>-0.649874</td>\n",
       "      <td>-3.423674</td>\n",
       "      <td>0.229748</td>\n",
       "      <td>-2.311088</td>\n",
       "      <td>-3.422217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>0.228994</td>\n",
       "      <td>-0.085453</td>\n",
       "      <td>0.876582</td>\n",
       "      <td>1.057401</td>\n",
       "      <td>-1.404015</td>\n",
       "      <td>-1.091965</td>\n",
       "      <td>0.639176</td>\n",
       "      <td>0.701332</td>\n",
       "      <td>-0.906577</td>\n",
       "      <td>-0.390940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.471415</td>\n",
       "      <td>1.024757</td>\n",
       "      <td>-1.796571</td>\n",
       "      <td>0.603161</td>\n",
       "      <td>0.862705</td>\n",
       "      <td>0.747234</td>\n",
       "      <td>3.275681</td>\n",
       "      <td>0.400372</td>\n",
       "      <td>-3.431031</td>\n",
       "      <td>2.370080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3          4         5         6   \\\n",
       "0    0.299403 -1.226624  1.498425 -1.176150   5.289853  0.208297  2.404498   \n",
       "1   -1.174176  0.332157  0.949919 -1.285328   2.199061 -0.151268 -0.427039   \n",
       "2    1.192222 -0.414371  0.067054 -2.233568   3.658881  0.089007  0.203439   \n",
       "3    1.573270 -0.580318 -0.866332 -0.603812   3.125716  0.870321 -0.161992   \n",
       "4   -0.613071 -0.644204  1.112558 -0.032397   3.490142 -0.011935  1.443521   \n",
       "..        ...       ...       ...       ...        ...       ...       ...   \n",
       "995 -0.310429  0.826811 -0.952245  0.768850   1.877520  1.320646  1.944609   \n",
       "996 -1.853879  0.246726  0.459921 -2.074267   7.599220 -0.138355 -4.501900   \n",
       "997  0.912748 -1.734039 -1.047035  0.217573  13.457812  0.162771 -2.250521   \n",
       "998  2.439780 -0.735511 -0.902426  1.365036 -10.430299 -0.856859  2.686474   \n",
       "999  0.228994 -0.085453  0.876582  1.057401  -1.404015 -1.091965  0.639176   \n",
       "\n",
       "           7         8         9   ...        30        31        32  \\\n",
       "0    1.594506 -0.051608  0.663234  ... -0.850465 -0.622990 -1.833057   \n",
       "1    2.619246 -0.765884 -0.093780  ... -0.819750  0.012037  2.038836   \n",
       "2   -4.219054 -1.184919 -1.240310  ... -0.604501  0.750054 -3.360521   \n",
       "3    4.499666  1.038741 -1.092716  ...  1.022959  1.275598 -3.480110   \n",
       "4   -4.290282 -1.761308  0.807652  ...  0.513906 -1.803473  0.518579   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995  1.191420 -0.127724  0.070937  ... -0.600411 -0.383792  0.745596   \n",
       "996  0.630634 -1.590533 -1.112949  ...  0.361736  0.240052 -0.856196   \n",
       "997  2.216161 -0.378326  0.642114  ...  1.195896 -1.073806 -2.754369   \n",
       "998  0.292035  0.585388 -0.876965  ...  2.262326 -0.039488  0.773876   \n",
       "999  0.701332 -0.906577 -0.390940  ...  0.471415  1.024757 -1.796571   \n",
       "\n",
       "           33        34        35        36        37        38        39  \n",
       "0    0.293024  3.552681  0.717611  3.305972 -2.715559 -2.682409  0.101050  \n",
       "1    0.468579 -0.517657  0.422326  0.803699  1.213219  1.382932 -1.817761  \n",
       "2    0.856988 -2.751451 -1.582735  1.672246  0.656438 -0.932473  2.987436  \n",
       "3   -1.065252  2.153133  1.563539  2.767117  0.215748  0.619645  1.883397  \n",
       "4   -0.205029 -4.744566 -1.520015  1.830651  0.870772 -1.894609  0.408332  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "995 -0.698598 -2.729937 -0.431535  0.372873  1.019092 -2.672811 -0.295141  \n",
       "996 -0.072481 -2.935896  0.582411 -2.613407  0.036687  2.809310  4.412567  \n",
       "997  1.814864 -4.190105 -1.116441 -2.100125  0.061513  0.895536  0.813686  \n",
       "998 -0.916066  2.604827 -0.649874 -3.423674  0.229748 -2.311088 -3.422217  \n",
       "999  0.603161  0.862705  0.747234  3.275681  0.400372 -3.431031  2.370080  \n",
       "\n",
       "[1000 rows x 40 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "0    1\n",
       "1    0\n",
       "2    0\n",
       "3    1\n",
       "4    0\n",
       "..  ..\n",
       "995  0\n",
       "996  1\n",
       "997  1\n",
       "998  0\n",
       "999  0\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.808909</td>\n",
       "      <td>-0.242894</td>\n",
       "      <td>-0.546421</td>\n",
       "      <td>0.255162</td>\n",
       "      <td>1.749736</td>\n",
       "      <td>-0.030458</td>\n",
       "      <td>-1.322071</td>\n",
       "      <td>3.578071</td>\n",
       "      <td>-0.667578</td>\n",
       "      <td>-0.884257</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261688</td>\n",
       "      <td>-0.224375</td>\n",
       "      <td>-1.675606</td>\n",
       "      <td>-0.479584</td>\n",
       "      <td>-0.244388</td>\n",
       "      <td>-0.672355</td>\n",
       "      <td>0.517860</td>\n",
       "      <td>0.010665</td>\n",
       "      <td>-0.419214</td>\n",
       "      <td>2.818387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.374101</td>\n",
       "      <td>0.537669</td>\n",
       "      <td>0.081063</td>\n",
       "      <td>0.756773</td>\n",
       "      <td>0.915231</td>\n",
       "      <td>2.557282</td>\n",
       "      <td>3.703187</td>\n",
       "      <td>1.673835</td>\n",
       "      <td>-0.764122</td>\n",
       "      <td>-1.228040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.969463</td>\n",
       "      <td>0.574154</td>\n",
       "      <td>-2.200519</td>\n",
       "      <td>-1.612240</td>\n",
       "      <td>0.179031</td>\n",
       "      <td>-2.924596</td>\n",
       "      <td>0.643610</td>\n",
       "      <td>-1.470939</td>\n",
       "      <td>-0.067408</td>\n",
       "      <td>-0.976265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.088370</td>\n",
       "      <td>0.154743</td>\n",
       "      <td>0.380716</td>\n",
       "      <td>-1.176126</td>\n",
       "      <td>1.699867</td>\n",
       "      <td>-0.258627</td>\n",
       "      <td>-1.384999</td>\n",
       "      <td>1.093584</td>\n",
       "      <td>1.596633</td>\n",
       "      <td>0.230631</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.769885</td>\n",
       "      <td>-0.005143</td>\n",
       "      <td>1.467490</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>-3.542981</td>\n",
       "      <td>0.814561</td>\n",
       "      <td>-1.652948</td>\n",
       "      <td>1.265866</td>\n",
       "      <td>-1.749248</td>\n",
       "      <td>1.773784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.685635</td>\n",
       "      <td>0.501283</td>\n",
       "      <td>1.873375</td>\n",
       "      <td>0.215224</td>\n",
       "      <td>-3.983468</td>\n",
       "      <td>-0.103637</td>\n",
       "      <td>4.136113</td>\n",
       "      <td>-0.225431</td>\n",
       "      <td>-1.515015</td>\n",
       "      <td>-1.071763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968609</td>\n",
       "      <td>2.386412</td>\n",
       "      <td>-0.131219</td>\n",
       "      <td>0.285646</td>\n",
       "      <td>2.302069</td>\n",
       "      <td>1.255588</td>\n",
       "      <td>-1.563090</td>\n",
       "      <td>-0.125258</td>\n",
       "      <td>-1.030761</td>\n",
       "      <td>-2.945329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.350867</td>\n",
       "      <td>0.721897</td>\n",
       "      <td>-0.477104</td>\n",
       "      <td>-1.748776</td>\n",
       "      <td>-2.627405</td>\n",
       "      <td>1.075433</td>\n",
       "      <td>4.954253</td>\n",
       "      <td>-3.293501</td>\n",
       "      <td>-0.760369</td>\n",
       "      <td>0.204360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260553</td>\n",
       "      <td>-2.045650</td>\n",
       "      <td>-2.173227</td>\n",
       "      <td>0.372992</td>\n",
       "      <td>0.450700</td>\n",
       "      <td>-0.211657</td>\n",
       "      <td>1.301359</td>\n",
       "      <td>-0.522164</td>\n",
       "      <td>2.484883</td>\n",
       "      <td>0.039213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8995</td>\n",
       "      <td>0.171644</td>\n",
       "      <td>-0.806952</td>\n",
       "      <td>-2.045671</td>\n",
       "      <td>0.021156</td>\n",
       "      <td>2.258491</td>\n",
       "      <td>0.429469</td>\n",
       "      <td>0.857187</td>\n",
       "      <td>0.972600</td>\n",
       "      <td>1.707492</td>\n",
       "      <td>1.676370</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.366312</td>\n",
       "      <td>0.276543</td>\n",
       "      <td>-0.732764</td>\n",
       "      <td>0.243930</td>\n",
       "      <td>-1.151233</td>\n",
       "      <td>-0.274298</td>\n",
       "      <td>0.573013</td>\n",
       "      <td>1.109814</td>\n",
       "      <td>-1.905965</td>\n",
       "      <td>1.457601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8996</td>\n",
       "      <td>1.168564</td>\n",
       "      <td>-0.911253</td>\n",
       "      <td>1.685492</td>\n",
       "      <td>0.867183</td>\n",
       "      <td>3.606170</td>\n",
       "      <td>-0.673875</td>\n",
       "      <td>-1.889365</td>\n",
       "      <td>0.411385</td>\n",
       "      <td>-0.206817</td>\n",
       "      <td>-0.705771</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557757</td>\n",
       "      <td>0.379841</td>\n",
       "      <td>-1.474198</td>\n",
       "      <td>-0.322943</td>\n",
       "      <td>1.964519</td>\n",
       "      <td>0.122384</td>\n",
       "      <td>0.678023</td>\n",
       "      <td>2.024129</td>\n",
       "      <td>0.386542</td>\n",
       "      <td>1.104493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8997</td>\n",
       "      <td>0.052274</td>\n",
       "      <td>-1.736558</td>\n",
       "      <td>-0.263699</td>\n",
       "      <td>-0.219329</td>\n",
       "      <td>8.918393</td>\n",
       "      <td>-1.258320</td>\n",
       "      <td>-3.361146</td>\n",
       "      <td>0.893366</td>\n",
       "      <td>-0.631669</td>\n",
       "      <td>1.887286</td>\n",
       "      <td>...</td>\n",
       "      <td>2.117847</td>\n",
       "      <td>-1.050824</td>\n",
       "      <td>0.182872</td>\n",
       "      <td>0.242725</td>\n",
       "      <td>0.670161</td>\n",
       "      <td>0.112752</td>\n",
       "      <td>-3.006949</td>\n",
       "      <td>1.179606</td>\n",
       "      <td>1.156340</td>\n",
       "      <td>-1.218561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8998</td>\n",
       "      <td>1.443659</td>\n",
       "      <td>0.651892</td>\n",
       "      <td>0.550724</td>\n",
       "      <td>-1.146664</td>\n",
       "      <td>2.621641</td>\n",
       "      <td>-0.867143</td>\n",
       "      <td>0.312742</td>\n",
       "      <td>1.078004</td>\n",
       "      <td>-1.212524</td>\n",
       "      <td>-0.028143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631480</td>\n",
       "      <td>1.186236</td>\n",
       "      <td>-1.098508</td>\n",
       "      <td>1.159658</td>\n",
       "      <td>-1.957241</td>\n",
       "      <td>0.482533</td>\n",
       "      <td>3.777669</td>\n",
       "      <td>-0.424954</td>\n",
       "      <td>1.333374</td>\n",
       "      <td>2.325271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8999</td>\n",
       "      <td>-0.429145</td>\n",
       "      <td>-0.110821</td>\n",
       "      <td>1.257230</td>\n",
       "      <td>-0.088150</td>\n",
       "      <td>-3.925020</td>\n",
       "      <td>-0.251062</td>\n",
       "      <td>-2.212032</td>\n",
       "      <td>-2.692121</td>\n",
       "      <td>-0.005468</td>\n",
       "      <td>0.186300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.384256</td>\n",
       "      <td>0.520610</td>\n",
       "      <td>1.594873</td>\n",
       "      <td>-0.352201</td>\n",
       "      <td>0.152333</td>\n",
       "      <td>-1.772249</td>\n",
       "      <td>0.758809</td>\n",
       "      <td>0.617485</td>\n",
       "      <td>1.464438</td>\n",
       "      <td>3.301445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     2.808909 -0.242894 -0.546421  0.255162  1.749736 -0.030458 -1.322071   \n",
       "1    -0.374101  0.537669  0.081063  0.756773  0.915231  2.557282  3.703187   \n",
       "2    -0.088370  0.154743  0.380716 -1.176126  1.699867 -0.258627 -1.384999   \n",
       "3    -0.685635  0.501283  1.873375  0.215224 -3.983468 -0.103637  4.136113   \n",
       "4     0.350867  0.721897 -0.477104 -1.748776 -2.627405  1.075433  4.954253   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8995  0.171644 -0.806952 -2.045671  0.021156  2.258491  0.429469  0.857187   \n",
       "8996  1.168564 -0.911253  1.685492  0.867183  3.606170 -0.673875 -1.889365   \n",
       "8997  0.052274 -1.736558 -0.263699 -0.219329  8.918393 -1.258320 -3.361146   \n",
       "8998  1.443659  0.651892  0.550724 -1.146664  2.621641 -0.867143  0.312742   \n",
       "8999 -0.429145 -0.110821  1.257230 -0.088150 -3.925020 -0.251062 -2.212032   \n",
       "\n",
       "            7         8         9   ...        30        31        32  \\\n",
       "0     3.578071 -0.667578 -0.884257  ... -0.261688 -0.224375 -1.675606   \n",
       "1     1.673835 -0.764122 -1.228040  ... -0.969463  0.574154 -2.200519   \n",
       "2     1.093584  1.596633  0.230631  ... -0.769885 -0.005143  1.467490   \n",
       "3    -0.225431 -1.515015 -1.071763  ...  0.968609  2.386412 -0.131219   \n",
       "4    -3.293501 -0.760369  0.204360  ...  0.260553 -2.045650 -2.173227   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "8995  0.972600  1.707492  1.676370  ... -1.366312  0.276543 -0.732764   \n",
       "8996  0.411385 -0.206817 -0.705771  ...  0.557757  0.379841 -1.474198   \n",
       "8997  0.893366 -0.631669  1.887286  ...  2.117847 -1.050824  0.182872   \n",
       "8998  1.078004 -1.212524 -0.028143  ...  0.631480  1.186236 -1.098508   \n",
       "8999 -2.692121 -0.005468  0.186300  ...  0.384256  0.520610  1.594873   \n",
       "\n",
       "            33        34        35        36        37        38        39  \n",
       "0    -0.479584 -0.244388 -0.672355  0.517860  0.010665 -0.419214  2.818387  \n",
       "1    -1.612240  0.179031 -2.924596  0.643610 -1.470939 -0.067408 -0.976265  \n",
       "2     0.483803 -3.542981  0.814561 -1.652948  1.265866 -1.749248  1.773784  \n",
       "3     0.285646  2.302069  1.255588 -1.563090 -0.125258 -1.030761 -2.945329  \n",
       "4     0.372992  0.450700 -0.211657  1.301359 -0.522164  2.484883  0.039213  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "8995  0.243930 -1.151233 -0.274298  0.573013  1.109814 -1.905965  1.457601  \n",
       "8996 -0.322943  1.964519  0.122384  0.678023  2.024129  0.386542  1.104493  \n",
       "8997  0.242725  0.670161  0.112752 -3.006949  1.179606  1.156340 -1.218561  \n",
       "8998  1.159658 -1.957241  0.482533  3.777669 -0.424954  1.333374  2.325271  \n",
       "8999 -0.352201  0.152333 -1.772249  0.758809  0.617485  1.464438  3.301445  \n",
       "\n",
       "[9000 rows x 40 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "            34, 35, 36, 37, 38, 39],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuastrain = train.columns\n",
    "continuastest = test.columns\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for var in continuas:\n",
    "    train[var] = train[var].astype('float64')\n",
    "    train[var] = scaler.fit_transform(train[var].values.reshape(-1, 1))\n",
    "    \n",
    "    test[var] = test[var].astype('float64')\n",
    "    test[var] = scaler.fit_transform(test[var].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = datos.drop(columns = ['Survived'])\n",
    "X_train = train\n",
    "y_train = trainLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.271694</td>\n",
       "      <td>-1.183412</td>\n",
       "      <td>1.555777</td>\n",
       "      <td>-1.210073</td>\n",
       "      <td>0.925265</td>\n",
       "      <td>0.217014</td>\n",
       "      <td>0.900554</td>\n",
       "      <td>0.731639</td>\n",
       "      <td>-0.077955</td>\n",
       "      <td>0.658265</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871410</td>\n",
       "      <td>-0.645377</td>\n",
       "      <td>-0.576450</td>\n",
       "      <td>0.298091</td>\n",
       "      <td>1.903665</td>\n",
       "      <td>0.679793</td>\n",
       "      <td>1.229941</td>\n",
       "      <td>-2.730228</td>\n",
       "      <td>-0.885572</td>\n",
       "      <td>-0.248677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-1.190511</td>\n",
       "      <td>0.351139</td>\n",
       "      <td>0.995287</td>\n",
       "      <td>-1.322617</td>\n",
       "      <td>0.243958</td>\n",
       "      <td>-0.146686</td>\n",
       "      <td>-0.436490</td>\n",
       "      <td>1.190929</td>\n",
       "      <td>-0.791829</td>\n",
       "      <td>-0.089025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.841033</td>\n",
       "      <td>-0.010905</td>\n",
       "      <td>1.152986</td>\n",
       "      <td>0.469876</td>\n",
       "      <td>-0.016107</td>\n",
       "      <td>0.386427</td>\n",
       "      <td>0.106214</td>\n",
       "      <td>1.209835</td>\n",
       "      <td>1.125967</td>\n",
       "      <td>-1.187240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.157622</td>\n",
       "      <td>-0.383785</td>\n",
       "      <td>0.093134</td>\n",
       "      <td>-2.300095</td>\n",
       "      <td>0.565748</td>\n",
       "      <td>0.096352</td>\n",
       "      <td>-0.138780</td>\n",
       "      <td>-1.874007</td>\n",
       "      <td>-1.210629</td>\n",
       "      <td>-1.220826</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.628156</td>\n",
       "      <td>0.726468</td>\n",
       "      <td>-1.258713</td>\n",
       "      <td>0.849944</td>\n",
       "      <td>-1.069674</td>\n",
       "      <td>-1.605605</td>\n",
       "      <td>0.496264</td>\n",
       "      <td>0.651454</td>\n",
       "      <td>-0.019700</td>\n",
       "      <td>1.163161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.535729</td>\n",
       "      <td>-0.547152</td>\n",
       "      <td>-0.860645</td>\n",
       "      <td>-0.620088</td>\n",
       "      <td>0.448222</td>\n",
       "      <td>0.886648</td>\n",
       "      <td>-0.311336</td>\n",
       "      <td>2.033736</td>\n",
       "      <td>1.011781</td>\n",
       "      <td>-1.075128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981376</td>\n",
       "      <td>1.251553</td>\n",
       "      <td>-1.312129</td>\n",
       "      <td>-1.031019</td>\n",
       "      <td>1.243569</td>\n",
       "      <td>1.520225</td>\n",
       "      <td>0.987951</td>\n",
       "      <td>0.209498</td>\n",
       "      <td>0.748291</td>\n",
       "      <td>0.623135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.633738</td>\n",
       "      <td>-0.610045</td>\n",
       "      <td>1.161480</td>\n",
       "      <td>-0.031054</td>\n",
       "      <td>0.528553</td>\n",
       "      <td>-0.005750</td>\n",
       "      <td>0.446783</td>\n",
       "      <td>-1.905931</td>\n",
       "      <td>-1.786693</td>\n",
       "      <td>0.800827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477930</td>\n",
       "      <td>-1.824829</td>\n",
       "      <td>0.473942</td>\n",
       "      <td>-0.189268</td>\n",
       "      <td>-2.009724</td>\n",
       "      <td>-1.543292</td>\n",
       "      <td>0.567400</td>\n",
       "      <td>0.866404</td>\n",
       "      <td>-0.495767</td>\n",
       "      <td>-0.098374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>-0.333432</td>\n",
       "      <td>0.838103</td>\n",
       "      <td>-0.948435</td>\n",
       "      <td>0.794897</td>\n",
       "      <td>0.173080</td>\n",
       "      <td>1.342151</td>\n",
       "      <td>0.683395</td>\n",
       "      <td>0.550975</td>\n",
       "      <td>-0.154028</td>\n",
       "      <td>0.073576</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.624111</td>\n",
       "      <td>-0.406388</td>\n",
       "      <td>0.575342</td>\n",
       "      <td>-0.672239</td>\n",
       "      <td>-1.059527</td>\n",
       "      <td>-0.461885</td>\n",
       "      <td>-0.087262</td>\n",
       "      <td>1.015150</td>\n",
       "      <td>-0.880823</td>\n",
       "      <td>-0.442470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>-1.864969</td>\n",
       "      <td>0.267035</td>\n",
       "      <td>0.494583</td>\n",
       "      <td>-2.135881</td>\n",
       "      <td>1.434322</td>\n",
       "      <td>-0.133624</td>\n",
       "      <td>-2.360628</td>\n",
       "      <td>0.299630</td>\n",
       "      <td>-1.616014</td>\n",
       "      <td>-1.095101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327437</td>\n",
       "      <td>0.216911</td>\n",
       "      <td>-0.140121</td>\n",
       "      <td>-0.059566</td>\n",
       "      <td>-1.156667</td>\n",
       "      <td>0.545472</td>\n",
       "      <td>-1.428348</td>\n",
       "      <td>0.029923</td>\n",
       "      <td>1.831741</td>\n",
       "      <td>1.860246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>0.880305</td>\n",
       "      <td>-1.682939</td>\n",
       "      <td>-1.045296</td>\n",
       "      <td>0.226623</td>\n",
       "      <td>2.725738</td>\n",
       "      <td>0.170964</td>\n",
       "      <td>-1.297533</td>\n",
       "      <td>1.010266</td>\n",
       "      <td>-0.404490</td>\n",
       "      <td>0.637416</td>\n",
       "      <td>...</td>\n",
       "      <td>1.152407</td>\n",
       "      <td>-1.095800</td>\n",
       "      <td>-0.987966</td>\n",
       "      <td>1.787251</td>\n",
       "      <td>-1.748213</td>\n",
       "      <td>-1.142341</td>\n",
       "      <td>-1.197842</td>\n",
       "      <td>0.054821</td>\n",
       "      <td>0.884803</td>\n",
       "      <td>0.099899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>2.395551</td>\n",
       "      <td>-0.699933</td>\n",
       "      <td>-0.897527</td>\n",
       "      <td>1.409465</td>\n",
       "      <td>-2.539946</td>\n",
       "      <td>-0.860389</td>\n",
       "      <td>1.033702</td>\n",
       "      <td>0.147869</td>\n",
       "      <td>0.558682</td>\n",
       "      <td>-0.862149</td>\n",
       "      <td>...</td>\n",
       "      <td>2.207089</td>\n",
       "      <td>-0.062384</td>\n",
       "      <td>0.587974</td>\n",
       "      <td>-0.885037</td>\n",
       "      <td>1.456610</td>\n",
       "      <td>-0.678805</td>\n",
       "      <td>-1.792225</td>\n",
       "      <td>0.223539</td>\n",
       "      <td>-0.701842</td>\n",
       "      <td>-1.972039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>0.201829</td>\n",
       "      <td>-0.059980</td>\n",
       "      <td>0.920348</td>\n",
       "      <td>1.092345</td>\n",
       "      <td>-0.550272</td>\n",
       "      <td>-1.098197</td>\n",
       "      <td>0.066974</td>\n",
       "      <td>0.331317</td>\n",
       "      <td>-0.932443</td>\n",
       "      <td>-0.382367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.435908</td>\n",
       "      <td>1.000931</td>\n",
       "      <td>-0.560153</td>\n",
       "      <td>0.601568</td>\n",
       "      <td>0.634940</td>\n",
       "      <td>0.709224</td>\n",
       "      <td>1.216338</td>\n",
       "      <td>0.394653</td>\n",
       "      <td>-1.255992</td>\n",
       "      <td>0.861190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0    0.271694 -1.183412  1.555777 -1.210073  0.925265  0.217014  0.900554   \n",
       "1   -1.190511  0.351139  0.995287 -1.322617  0.243958 -0.146686 -0.436490   \n",
       "2    1.157622 -0.383785  0.093134 -2.300095  0.565748  0.096352 -0.138780   \n",
       "3    1.535729 -0.547152 -0.860645 -0.620088  0.448222  0.886648 -0.311336   \n",
       "4   -0.633738 -0.610045  1.161480 -0.031054  0.528553 -0.005750  0.446783   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "995 -0.333432  0.838103 -0.948435  0.794897  0.173080  1.342151  0.683395   \n",
       "996 -1.864969  0.267035  0.494583 -2.135881  1.434322 -0.133624 -2.360628   \n",
       "997  0.880305 -1.682939 -1.045296  0.226623  2.725738  0.170964 -1.297533   \n",
       "998  2.395551 -0.699933 -0.897527  1.409465 -2.539946 -0.860389  1.033702   \n",
       "999  0.201829 -0.059980  0.920348  1.092345 -0.550272 -1.098197  0.066974   \n",
       "\n",
       "           7         8         9   ...        30        31        32  \\\n",
       "0    0.731639 -0.077955  0.658265  ... -0.871410 -0.645377 -0.576450   \n",
       "1    1.190929 -0.791829 -0.089025  ... -0.841033 -0.010905  1.152986   \n",
       "2   -1.874007 -1.210629 -1.220826  ... -0.628156  0.726468 -1.258713   \n",
       "3    2.033736  1.011781 -1.075128  ...  0.981376  1.251553 -1.312129   \n",
       "4   -1.905931 -1.786693  0.800827  ...  0.477930 -1.824829  0.473942   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995  0.550975 -0.154028  0.073576  ... -0.624111 -0.406388  0.575342   \n",
       "996  0.299630 -1.616014 -1.095101  ...  0.327437  0.216911 -0.140121   \n",
       "997  1.010266 -0.404490  0.637416  ...  1.152407 -1.095800 -0.987966   \n",
       "998  0.147869  0.558682 -0.862149  ...  2.207089 -0.062384  0.587974   \n",
       "999  0.331317 -0.932443 -0.382367  ...  0.435908  1.000931 -0.560153   \n",
       "\n",
       "           33        34        35        36        37        38        39  \n",
       "0    0.298091  1.903665  0.679793  1.229941 -2.730228 -0.885572 -0.248677  \n",
       "1    0.469876 -0.016107  0.386427  0.106214  1.209835  1.125967 -1.187240  \n",
       "2    0.849944 -1.069674 -1.605605  0.496264  0.651454 -0.019700  1.163161  \n",
       "3   -1.031019  1.243569  1.520225  0.987951  0.209498  0.748291  0.623135  \n",
       "4   -0.189268 -2.009724 -1.543292  0.567400  0.866404 -0.495767 -0.098374  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "995 -0.672239 -1.059527 -0.461885 -0.087262  1.015150 -0.880823 -0.442470  \n",
       "996 -0.059566 -1.156667  0.545472 -1.428348  0.029923  1.831741  1.860246  \n",
       "997  1.787251 -1.748213 -1.142341 -1.197842  0.054821  0.884803  0.099899  \n",
       "998 -0.885037  1.456610 -0.678805 -1.792225  0.223539 -0.701842 -1.972039  \n",
       "999  0.601568  0.634940  0.709224  1.216338  0.394653 -1.255992  0.861190  \n",
       "\n",
       "[1000 rows x 40 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7083 - binary_accuracy: 0.4737 - val_loss: 0.6992 - val_binary_accuracy: 0.4850\n",
      "Epoch 2/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6911 - binary_accuracy: 0.5225 - val_loss: 0.6891 - val_binary_accuracy: 0.5500\n",
      "Epoch 3/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6789 - binary_accuracy: 0.5875 - val_loss: 0.6810 - val_binary_accuracy: 0.5650\n",
      "Epoch 4/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6646 - binary_accuracy: 0.6100 - val_loss: 0.6766 - val_binary_accuracy: 0.5650\n",
      "Epoch 5/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6519 - binary_accuracy: 0.6712 - val_loss: 0.6631 - val_binary_accuracy: 0.6100\n",
      "Epoch 6/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6352 - binary_accuracy: 0.6950 - val_loss: 0.6494 - val_binary_accuracy: 0.6450\n",
      "Epoch 7/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6173 - binary_accuracy: 0.7150 - val_loss: 0.6329 - val_binary_accuracy: 0.6900\n",
      "Epoch 8/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5969 - binary_accuracy: 0.7375 - val_loss: 0.6145 - val_binary_accuracy: 0.7100\n",
      "Epoch 9/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5745 - binary_accuracy: 0.7513 - val_loss: 0.5999 - val_binary_accuracy: 0.7450\n",
      "Epoch 10/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5509 - binary_accuracy: 0.7688 - val_loss: 0.5783 - val_binary_accuracy: 0.7450\n",
      "Epoch 11/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5258 - binary_accuracy: 0.7775 - val_loss: 0.5624 - val_binary_accuracy: 0.7700\n",
      "Epoch 12/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5026 - binary_accuracy: 0.7862 - val_loss: 0.5449 - val_binary_accuracy: 0.7750\n",
      "Epoch 13/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4810 - binary_accuracy: 0.7900 - val_loss: 0.5290 - val_binary_accuracy: 0.7800\n",
      "Epoch 14/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4620 - binary_accuracy: 0.8025 - val_loss: 0.5175 - val_binary_accuracy: 0.7750\n",
      "Epoch 15/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4464 - binary_accuracy: 0.7987 - val_loss: 0.5063 - val_binary_accuracy: 0.7950\n",
      "Epoch 16/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4333 - binary_accuracy: 0.8087 - val_loss: 0.4994 - val_binary_accuracy: 0.7900\n",
      "Epoch 17/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4230 - binary_accuracy: 0.8150 - val_loss: 0.4974 - val_binary_accuracy: 0.7950\n",
      "Epoch 18/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4137 - binary_accuracy: 0.8175 - val_loss: 0.4938 - val_binary_accuracy: 0.8000\n",
      "Epoch 19/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4064 - binary_accuracy: 0.8188 - val_loss: 0.4912 - val_binary_accuracy: 0.8050\n",
      "Epoch 20/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4005 - binary_accuracy: 0.8288 - val_loss: 0.4871 - val_binary_accuracy: 0.8050\n",
      "Epoch 21/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3955 - binary_accuracy: 0.8250 - val_loss: 0.4862 - val_binary_accuracy: 0.8050\n",
      "Epoch 22/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3909 - binary_accuracy: 0.8300 - val_loss: 0.4798 - val_binary_accuracy: 0.8000\n",
      "Epoch 23/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3883 - binary_accuracy: 0.8375 - val_loss: 0.4797 - val_binary_accuracy: 0.8050\n",
      "Epoch 24/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3847 - binary_accuracy: 0.8300 - val_loss: 0.4773 - val_binary_accuracy: 0.8000\n",
      "Epoch 25/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3816 - binary_accuracy: 0.8388 - val_loss: 0.4782 - val_binary_accuracy: 0.8100\n",
      "Epoch 26/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3793 - binary_accuracy: 0.8400 - val_loss: 0.4776 - val_binary_accuracy: 0.8100\n",
      "Epoch 27/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3767 - binary_accuracy: 0.8350 - val_loss: 0.4741 - val_binary_accuracy: 0.8100\n",
      "Epoch 28/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3759 - binary_accuracy: 0.8388 - val_loss: 0.4739 - val_binary_accuracy: 0.8100\n",
      "Epoch 29/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3749 - binary_accuracy: 0.8462 - val_loss: 0.4757 - val_binary_accuracy: 0.8150\n",
      "Epoch 30/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3704 - binary_accuracy: 0.8375 - val_loss: 0.4713 - val_binary_accuracy: 0.8050\n",
      "Epoch 31/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3700 - binary_accuracy: 0.8462 - val_loss: 0.4701 - val_binary_accuracy: 0.8000\n",
      "Epoch 32/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3703 - binary_accuracy: 0.8388 - val_loss: 0.4753 - val_binary_accuracy: 0.8200\n",
      "Epoch 33/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3675 - binary_accuracy: 0.8438 - val_loss: 0.4783 - val_binary_accuracy: 0.8200\n",
      "Epoch 34/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3665 - binary_accuracy: 0.8475 - val_loss: 0.4767 - val_binary_accuracy: 0.8200\n",
      "Epoch 35/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3658 - binary_accuracy: 0.8462 - val_loss: 0.4725 - val_binary_accuracy: 0.8150\n",
      "Epoch 36/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3654 - binary_accuracy: 0.8475 - val_loss: 0.4716 - val_binary_accuracy: 0.8150\n",
      "Epoch 37/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3634 - binary_accuracy: 0.8450 - val_loss: 0.4700 - val_binary_accuracy: 0.8150\n",
      "Epoch 38/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3625 - binary_accuracy: 0.8450 - val_loss: 0.4758 - val_binary_accuracy: 0.8150\n",
      "Epoch 39/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3611 - binary_accuracy: 0.8438 - val_loss: 0.4675 - val_binary_accuracy: 0.8100\n",
      "Epoch 40/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3598 - binary_accuracy: 0.8450 - val_loss: 0.4750 - val_binary_accuracy: 0.8150\n",
      "Epoch 41/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3568 - binary_accuracy: 0.8338 - val_loss: 0.4649 - val_binary_accuracy: 0.8100\n",
      "Epoch 42/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3581 - binary_accuracy: 0.8462 - val_loss: 0.4661 - val_binary_accuracy: 0.8150\n",
      "Epoch 43/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3561 - binary_accuracy: 0.8413 - val_loss: 0.4680 - val_binary_accuracy: 0.8100\n",
      "Epoch 44/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3540 - binary_accuracy: 0.8500 - val_loss: 0.4691 - val_binary_accuracy: 0.8150\n",
      "Epoch 45/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3535 - binary_accuracy: 0.8450 - val_loss: 0.4653 - val_binary_accuracy: 0.8150\n",
      "Epoch 46/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3541 - binary_accuracy: 0.8438 - val_loss: 0.4647 - val_binary_accuracy: 0.8150\n",
      "Epoch 47/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3518 - binary_accuracy: 0.8438 - val_loss: 0.4675 - val_binary_accuracy: 0.8100\n",
      "Epoch 48/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3501 - binary_accuracy: 0.8438 - val_loss: 0.4629 - val_binary_accuracy: 0.8200\n",
      "Epoch 49/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3496 - binary_accuracy: 0.8450 - val_loss: 0.4638 - val_binary_accuracy: 0.8150\n",
      "Epoch 50/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3491 - binary_accuracy: 0.8450 - val_loss: 0.4632 - val_binary_accuracy: 0.8150\n",
      "Epoch 51/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3478 - binary_accuracy: 0.8413 - val_loss: 0.4624 - val_binary_accuracy: 0.8150\n",
      "Epoch 52/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3474 - binary_accuracy: 0.8475 - val_loss: 0.4617 - val_binary_accuracy: 0.8100\n",
      "Epoch 53/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3458 - binary_accuracy: 0.8500 - val_loss: 0.4628 - val_binary_accuracy: 0.8150\n",
      "Epoch 54/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3439 - binary_accuracy: 0.8438 - val_loss: 0.4672 - val_binary_accuracy: 0.8100\n",
      "Epoch 55/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3424 - binary_accuracy: 0.8425 - val_loss: 0.4604 - val_binary_accuracy: 0.8100\n",
      "Epoch 56/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3423 - binary_accuracy: 0.8462 - val_loss: 0.4603 - val_binary_accuracy: 0.8100\n",
      "Epoch 57/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3413 - binary_accuracy: 0.8525 - val_loss: 0.4626 - val_binary_accuracy: 0.8150\n",
      "Epoch 58/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3395 - binary_accuracy: 0.8475 - val_loss: 0.4624 - val_binary_accuracy: 0.8150\n",
      "Epoch 59/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3383 - binary_accuracy: 0.8475 - val_loss: 0.4620 - val_binary_accuracy: 0.8100\n",
      "Epoch 60/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3379 - binary_accuracy: 0.8500 - val_loss: 0.4627 - val_binary_accuracy: 0.8100\n",
      "Epoch 61/200\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.3374 - binary_accuracy: 0.8413 - val_loss: 0.4634 - val_binary_accuracy: 0.8100\n",
      "Epoch 62/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3358 - binary_accuracy: 0.8475 - val_loss: 0.4623 - val_binary_accuracy: 0.8050\n",
      "Epoch 63/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3340 - binary_accuracy: 0.8487 - val_loss: 0.4595 - val_binary_accuracy: 0.8050\n",
      "Epoch 64/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3327 - binary_accuracy: 0.8537 - val_loss: 0.4634 - val_binary_accuracy: 0.8050\n",
      "Epoch 65/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3310 - binary_accuracy: 0.8550 - val_loss: 0.4637 - val_binary_accuracy: 0.8150\n",
      "Epoch 66/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3302 - binary_accuracy: 0.8525 - val_loss: 0.4591 - val_binary_accuracy: 0.8050\n",
      "Epoch 67/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3289 - binary_accuracy: 0.8587 - val_loss: 0.4597 - val_binary_accuracy: 0.8050\n",
      "Epoch 68/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3276 - binary_accuracy: 0.8525 - val_loss: 0.4615 - val_binary_accuracy: 0.8050\n",
      "Epoch 69/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3256 - binary_accuracy: 0.8475 - val_loss: 0.4555 - val_binary_accuracy: 0.8100\n",
      "Epoch 70/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3249 - binary_accuracy: 0.8525 - val_loss: 0.4576 - val_binary_accuracy: 0.8050\n",
      "Epoch 71/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3237 - binary_accuracy: 0.8512 - val_loss: 0.4551 - val_binary_accuracy: 0.8100\n",
      "Epoch 72/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3228 - binary_accuracy: 0.8562 - val_loss: 0.4556 - val_binary_accuracy: 0.8050\n",
      "Epoch 73/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3216 - binary_accuracy: 0.8537 - val_loss: 0.4590 - val_binary_accuracy: 0.8100\n",
      "Epoch 74/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3198 - binary_accuracy: 0.8575 - val_loss: 0.4595 - val_binary_accuracy: 0.8100\n",
      "Epoch 75/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3185 - binary_accuracy: 0.8600 - val_loss: 0.4557 - val_binary_accuracy: 0.8050\n",
      "Epoch 76/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3175 - binary_accuracy: 0.8575 - val_loss: 0.4579 - val_binary_accuracy: 0.8100\n",
      "Epoch 77/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3172 - binary_accuracy: 0.8600 - val_loss: 0.4549 - val_binary_accuracy: 0.8050\n",
      "Epoch 78/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3160 - binary_accuracy: 0.8575 - val_loss: 0.4553 - val_binary_accuracy: 0.8050\n",
      "Epoch 79/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3138 - binary_accuracy: 0.8550 - val_loss: 0.4516 - val_binary_accuracy: 0.8050\n",
      "Epoch 80/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3128 - binary_accuracy: 0.8637 - val_loss: 0.4516 - val_binary_accuracy: 0.8050\n",
      "Epoch 81/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3107 - binary_accuracy: 0.8612 - val_loss: 0.4647 - val_binary_accuracy: 0.8100\n",
      "Epoch 82/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3106 - binary_accuracy: 0.8575 - val_loss: 0.4549 - val_binary_accuracy: 0.8050\n",
      "Epoch 83/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3069 - binary_accuracy: 0.8625 - val_loss: 0.4599 - val_binary_accuracy: 0.8050\n",
      "Epoch 84/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3063 - binary_accuracy: 0.8612 - val_loss: 0.4547 - val_binary_accuracy: 0.8050\n",
      "Epoch 85/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3059 - binary_accuracy: 0.8587 - val_loss: 0.4511 - val_binary_accuracy: 0.8150\n",
      "Epoch 86/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3045 - binary_accuracy: 0.8625 - val_loss: 0.4518 - val_binary_accuracy: 0.8150\n",
      "Epoch 87/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3022 - binary_accuracy: 0.8687 - val_loss: 0.4491 - val_binary_accuracy: 0.8100\n",
      "Epoch 88/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3021 - binary_accuracy: 0.8612 - val_loss: 0.4498 - val_binary_accuracy: 0.8150\n",
      "Epoch 89/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2985 - binary_accuracy: 0.8600 - val_loss: 0.4588 - val_binary_accuracy: 0.8050\n",
      "Epoch 90/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2974 - binary_accuracy: 0.8725 - val_loss: 0.4506 - val_binary_accuracy: 0.8100\n",
      "Epoch 91/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2977 - binary_accuracy: 0.8700 - val_loss: 0.4515 - val_binary_accuracy: 0.8150\n",
      "Epoch 92/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2956 - binary_accuracy: 0.8650 - val_loss: 0.4516 - val_binary_accuracy: 0.8150\n",
      "Epoch 93/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2937 - binary_accuracy: 0.8650 - val_loss: 0.4493 - val_binary_accuracy: 0.8150\n",
      "Epoch 94/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2930 - binary_accuracy: 0.8700 - val_loss: 0.4502 - val_binary_accuracy: 0.8150\n",
      "Epoch 95/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2917 - binary_accuracy: 0.8687 - val_loss: 0.4501 - val_binary_accuracy: 0.8150\n",
      "Epoch 96/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2906 - binary_accuracy: 0.8687 - val_loss: 0.4526 - val_binary_accuracy: 0.8100\n",
      "Epoch 97/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2883 - binary_accuracy: 0.8725 - val_loss: 0.4545 - val_binary_accuracy: 0.8100\n",
      "Epoch 98/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2877 - binary_accuracy: 0.8700 - val_loss: 0.4508 - val_binary_accuracy: 0.8100\n",
      "Epoch 99/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2859 - binary_accuracy: 0.8712 - val_loss: 0.4523 - val_binary_accuracy: 0.8150\n",
      "Epoch 100/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2840 - binary_accuracy: 0.8712 - val_loss: 0.4554 - val_binary_accuracy: 0.8100\n",
      "Epoch 101/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2829 - binary_accuracy: 0.8700 - val_loss: 0.4491 - val_binary_accuracy: 0.8150\n",
      "Epoch 102/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2823 - binary_accuracy: 0.8800 - val_loss: 0.4525 - val_binary_accuracy: 0.8200\n",
      "Epoch 103/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2794 - binary_accuracy: 0.8750 - val_loss: 0.4502 - val_binary_accuracy: 0.8100\n",
      "Epoch 104/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2790 - binary_accuracy: 0.8775 - val_loss: 0.4526 - val_binary_accuracy: 0.8100\n",
      "Epoch 105/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2778 - binary_accuracy: 0.8788 - val_loss: 0.4514 - val_binary_accuracy: 0.8100\n",
      "Epoch 106/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2766 - binary_accuracy: 0.8788 - val_loss: 0.4598 - val_binary_accuracy: 0.8050\n",
      "Epoch 107/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2734 - binary_accuracy: 0.8825 - val_loss: 0.4508 - val_binary_accuracy: 0.8150\n",
      "Epoch 108/200\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2740 - binary_accuracy: 0.8800 - val_loss: 0.4541 - val_binary_accuracy: 0.8100\n",
      "Epoch 109/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2710 - binary_accuracy: 0.8850 - val_loss: 0.4520 - val_binary_accuracy: 0.8050\n",
      "Epoch 110/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2704 - binary_accuracy: 0.8838 - val_loss: 0.4554 - val_binary_accuracy: 0.8050\n",
      "Epoch 111/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2687 - binary_accuracy: 0.8850 - val_loss: 0.4583 - val_binary_accuracy: 0.8100\n",
      "Epoch 112/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2681 - binary_accuracy: 0.8825 - val_loss: 0.4551 - val_binary_accuracy: 0.8050\n",
      "Epoch 113/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2671 - binary_accuracy: 0.8800 - val_loss: 0.4521 - val_binary_accuracy: 0.8050\n",
      "Epoch 114/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2646 - binary_accuracy: 0.8813 - val_loss: 0.4527 - val_binary_accuracy: 0.8100\n",
      "Epoch 115/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2631 - binary_accuracy: 0.8838 - val_loss: 0.4553 - val_binary_accuracy: 0.8050\n",
      "Epoch 116/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2598 - binary_accuracy: 0.8888 - val_loss: 0.4558 - val_binary_accuracy: 0.8050\n",
      "Epoch 117/200\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2602 - binary_accuracy: 0.8888 - val_loss: 0.4533 - val_binary_accuracy: 0.8200\n",
      "Epoch 118/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2585 - binary_accuracy: 0.8900 - val_loss: 0.4587 - val_binary_accuracy: 0.8000\n",
      "Epoch 119/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2568 - binary_accuracy: 0.8875 - val_loss: 0.4544 - val_binary_accuracy: 0.8200\n",
      "Epoch 120/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2550 - binary_accuracy: 0.8913 - val_loss: 0.4557 - val_binary_accuracy: 0.8000\n",
      "Epoch 121/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2543 - binary_accuracy: 0.8875 - val_loss: 0.4568 - val_binary_accuracy: 0.7950\n",
      "Epoch 122/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2523 - binary_accuracy: 0.8975 - val_loss: 0.4552 - val_binary_accuracy: 0.8050\n",
      "Epoch 123/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2514 - binary_accuracy: 0.8950 - val_loss: 0.4584 - val_binary_accuracy: 0.8000\n",
      "Epoch 124/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2503 - binary_accuracy: 0.8913 - val_loss: 0.4566 - val_binary_accuracy: 0.8050\n",
      "Epoch 125/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2478 - binary_accuracy: 0.8938 - val_loss: 0.4571 - val_binary_accuracy: 0.8050\n",
      "Epoch 126/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2470 - binary_accuracy: 0.8938 - val_loss: 0.4574 - val_binary_accuracy: 0.8050\n",
      "Epoch 127/200\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2467 - binary_accuracy: 0.8950 - val_loss: 0.4563 - val_binary_accuracy: 0.8000\n",
      "Epoch 128/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2432 - binary_accuracy: 0.9000 - val_loss: 0.4566 - val_binary_accuracy: 0.8000\n",
      "Epoch 129/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2423 - binary_accuracy: 0.9025 - val_loss: 0.4601 - val_binary_accuracy: 0.8100\n",
      "Epoch 130/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2395 - binary_accuracy: 0.9050 - val_loss: 0.4673 - val_binary_accuracy: 0.8000\n",
      "Epoch 131/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2414 - binary_accuracy: 0.8988 - val_loss: 0.4592 - val_binary_accuracy: 0.8050\n",
      "Epoch 132/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2361 - binary_accuracy: 0.9062 - val_loss: 0.4656 - val_binary_accuracy: 0.8000\n",
      "Epoch 133/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2362 - binary_accuracy: 0.9025 - val_loss: 0.4589 - val_binary_accuracy: 0.8000\n",
      "Epoch 134/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2353 - binary_accuracy: 0.9000 - val_loss: 0.4601 - val_binary_accuracy: 0.8000\n",
      "Epoch 135/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2335 - binary_accuracy: 0.9075 - val_loss: 0.4595 - val_binary_accuracy: 0.8000\n",
      "Epoch 136/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2328 - binary_accuracy: 0.9050 - val_loss: 0.4612 - val_binary_accuracy: 0.8000\n",
      "Epoch 137/200\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1875 - binary_accuracy: 0.937 - 0s 1ms/step - loss: 0.2307 - binary_accuracy: 0.9087 - val_loss: 0.4608 - val_binary_accuracy: 0.8000\n",
      "Epoch 138/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2288 - binary_accuracy: 0.9038 - val_loss: 0.4625 - val_binary_accuracy: 0.8000\n",
      "Epoch 139/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2283 - binary_accuracy: 0.9075 - val_loss: 0.4630 - val_binary_accuracy: 0.8050\n",
      "Epoch 140/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2265 - binary_accuracy: 0.9075 - val_loss: 0.4605 - val_binary_accuracy: 0.8000\n",
      "Epoch 141/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2255 - binary_accuracy: 0.9087 - val_loss: 0.4615 - val_binary_accuracy: 0.8000\n",
      "Epoch 142/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2225 - binary_accuracy: 0.9062 - val_loss: 0.4627 - val_binary_accuracy: 0.8050\n",
      "Epoch 143/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2242 - binary_accuracy: 0.9050 - val_loss: 0.4632 - val_binary_accuracy: 0.8050\n",
      "Epoch 144/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2207 - binary_accuracy: 0.9087 - val_loss: 0.4629 - val_binary_accuracy: 0.8050\n",
      "Epoch 145/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2198 - binary_accuracy: 0.9150 - val_loss: 0.4649 - val_binary_accuracy: 0.8050\n",
      "Epoch 146/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2190 - binary_accuracy: 0.9137 - val_loss: 0.4648 - val_binary_accuracy: 0.8050\n",
      "Epoch 147/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2172 - binary_accuracy: 0.9137 - val_loss: 0.4683 - val_binary_accuracy: 0.8100\n",
      "Epoch 148/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2146 - binary_accuracy: 0.9162 - val_loss: 0.4659 - val_binary_accuracy: 0.8050\n",
      "Epoch 149/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2130 - binary_accuracy: 0.9200 - val_loss: 0.4644 - val_binary_accuracy: 0.8100\n",
      "Epoch 150/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2115 - binary_accuracy: 0.9187 - val_loss: 0.4666 - val_binary_accuracy: 0.7950\n",
      "Epoch 151/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2109 - binary_accuracy: 0.9150 - val_loss: 0.4651 - val_binary_accuracy: 0.8050\n",
      "Epoch 152/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2110 - binary_accuracy: 0.9162 - val_loss: 0.4656 - val_binary_accuracy: 0.8050\n",
      "Epoch 153/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2077 - binary_accuracy: 0.9162 - val_loss: 0.4660 - val_binary_accuracy: 0.8050\n",
      "Epoch 154/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2059 - binary_accuracy: 0.9175 - val_loss: 0.4682 - val_binary_accuracy: 0.8050\n",
      "Epoch 155/200\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2057 - binary_accuracy: 0.9212 - val_loss: 0.4668 - val_binary_accuracy: 0.8150\n",
      "Epoch 156/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2038 - binary_accuracy: 0.9200 - val_loss: 0.4686 - val_binary_accuracy: 0.8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2005 - binary_accuracy: 0.9212 - val_loss: 0.4713 - val_binary_accuracy: 0.8100\n",
      "Epoch 158/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2014 - binary_accuracy: 0.9225 - val_loss: 0.4700 - val_binary_accuracy: 0.8100\n",
      "Epoch 159/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1985 - binary_accuracy: 0.9225 - val_loss: 0.4700 - val_binary_accuracy: 0.8100\n",
      "Epoch 160/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1966 - binary_accuracy: 0.9212 - val_loss: 0.4718 - val_binary_accuracy: 0.8100\n",
      "Epoch 161/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1955 - binary_accuracy: 0.9200 - val_loss: 0.4733 - val_binary_accuracy: 0.8150\n",
      "Epoch 162/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1941 - binary_accuracy: 0.9312 - val_loss: 0.4742 - val_binary_accuracy: 0.8100\n",
      "Epoch 163/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1928 - binary_accuracy: 0.9287 - val_loss: 0.4728 - val_binary_accuracy: 0.8150\n",
      "Epoch 164/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1905 - binary_accuracy: 0.9312 - val_loss: 0.4732 - val_binary_accuracy: 0.8050\n",
      "Epoch 165/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1904 - binary_accuracy: 0.9312 - val_loss: 0.4725 - val_binary_accuracy: 0.8100\n",
      "Epoch 166/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1877 - binary_accuracy: 0.9287 - val_loss: 0.4852 - val_binary_accuracy: 0.8000\n",
      "Epoch 167/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1889 - binary_accuracy: 0.9250 - val_loss: 0.4754 - val_binary_accuracy: 0.8050\n",
      "Epoch 168/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1858 - binary_accuracy: 0.9262 - val_loss: 0.4774 - val_binary_accuracy: 0.8050\n",
      "Epoch 169/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1842 - binary_accuracy: 0.9275 - val_loss: 0.4759 - val_binary_accuracy: 0.8050\n",
      "Epoch 170/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1814 - binary_accuracy: 0.9287 - val_loss: 0.4894 - val_binary_accuracy: 0.8000\n",
      "Epoch 171/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1822 - binary_accuracy: 0.9300 - val_loss: 0.4784 - val_binary_accuracy: 0.8050\n",
      "Epoch 172/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1787 - binary_accuracy: 0.9337 - val_loss: 0.4819 - val_binary_accuracy: 0.7950\n",
      "Epoch 173/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1782 - binary_accuracy: 0.9312 - val_loss: 0.4836 - val_binary_accuracy: 0.8150\n",
      "Epoch 174/200\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1764 - binary_accuracy: 0.9325 - val_loss: 0.4874 - val_binary_accuracy: 0.8050\n",
      "Epoch 175/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1757 - binary_accuracy: 0.9362 - val_loss: 0.4832 - val_binary_accuracy: 0.8150\n",
      "Epoch 176/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1741 - binary_accuracy: 0.9325 - val_loss: 0.4848 - val_binary_accuracy: 0.8100\n",
      "Epoch 177/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1710 - binary_accuracy: 0.9337 - val_loss: 0.4869 - val_binary_accuracy: 0.8150\n",
      "Epoch 178/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1714 - binary_accuracy: 0.9362 - val_loss: 0.4896 - val_binary_accuracy: 0.8000\n",
      "Epoch 179/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1707 - binary_accuracy: 0.9350 - val_loss: 0.4890 - val_binary_accuracy: 0.8150\n",
      "Epoch 180/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1684 - binary_accuracy: 0.9350 - val_loss: 0.4892 - val_binary_accuracy: 0.8150\n",
      "Epoch 181/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1657 - binary_accuracy: 0.9375 - val_loss: 0.4895 - val_binary_accuracy: 0.8100\n",
      "Epoch 182/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1642 - binary_accuracy: 0.9400 - val_loss: 0.4901 - val_binary_accuracy: 0.8100\n",
      "Epoch 183/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1628 - binary_accuracy: 0.9413 - val_loss: 0.4917 - val_binary_accuracy: 0.8100\n",
      "Epoch 184/200\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1621 - binary_accuracy: 0.9413 - val_loss: 0.4944 - val_binary_accuracy: 0.8000\n",
      "Epoch 185/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1598 - binary_accuracy: 0.9425 - val_loss: 0.4939 - val_binary_accuracy: 0.7900\n",
      "Epoch 186/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1582 - binary_accuracy: 0.9400 - val_loss: 0.4979 - val_binary_accuracy: 0.8050\n",
      "Epoch 187/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1584 - binary_accuracy: 0.9413 - val_loss: 0.5003 - val_binary_accuracy: 0.8050\n",
      "Epoch 188/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1561 - binary_accuracy: 0.9413 - val_loss: 0.4952 - val_binary_accuracy: 0.7900\n",
      "Epoch 189/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1549 - binary_accuracy: 0.9362 - val_loss: 0.4948 - val_binary_accuracy: 0.7900\n",
      "Epoch 190/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1536 - binary_accuracy: 0.9438 - val_loss: 0.4999 - val_binary_accuracy: 0.7950\n",
      "Epoch 191/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1515 - binary_accuracy: 0.9463 - val_loss: 0.4992 - val_binary_accuracy: 0.7850\n",
      "Epoch 192/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1494 - binary_accuracy: 0.9450 - val_loss: 0.5002 - val_binary_accuracy: 0.7850\n",
      "Epoch 193/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1481 - binary_accuracy: 0.9425 - val_loss: 0.5052 - val_binary_accuracy: 0.7900\n",
      "Epoch 194/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1465 - binary_accuracy: 0.9500 - val_loss: 0.5070 - val_binary_accuracy: 0.8000\n",
      "Epoch 195/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1460 - binary_accuracy: 0.9463 - val_loss: 0.5040 - val_binary_accuracy: 0.7900\n",
      "Epoch 196/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1439 - binary_accuracy: 0.9475 - val_loss: 0.5057 - val_binary_accuracy: 0.7850\n",
      "Epoch 197/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1416 - binary_accuracy: 0.9475 - val_loss: 0.5107 - val_binary_accuracy: 0.7950\n",
      "Epoch 198/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1413 - binary_accuracy: 0.9513 - val_loss: 0.5089 - val_binary_accuracy: 0.7900\n",
      "Epoch 199/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1396 - binary_accuracy: 0.9500 - val_loss: 0.5123 - val_binary_accuracy: 0.7800\n",
      "Epoch 200/200\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1398 - binary_accuracy: 0.9538 - val_loss: 0.5151 - val_binary_accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim = X_train.shape[1], activation = 'sigmoid'))\n",
    "model.add(Dense(15, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = SGD(lr = 0.05), metrics = ['binary_accuracy'])\n",
    "history = model.fit(X_train, y_train, validation_split = 0.2, epochs = 200, batch_size = 32, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfaux = pd.DataFrame(columns = ['Epocas','Capa1 - Funcion de activacion', 'Capa2 - Funcion de activacion', 'Capa3 - Funcion de activacion', 'learning_rate', 'batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo(epocas, act_capa1, act_capa2, act_capa3, learning_rate, batch_size):\n",
    "    dfaux = pd.DataFrame(columns = ['Epocas','Capa1 - Funcion de activacion', 'Capa2 - Funcion de activacion', 'Capa3 - Funcion de activacion', 'learning_rate', 'batch_size', 'precision', 'perdida'])\n",
    "    i = 0\n",
    "    for epoca in epocas:\n",
    "        for act1 in act_capa1:\n",
    "            for act2 in act_capa2:\n",
    "                for act3 in act_capa3:\n",
    "                    for lrate in learning_rate:\n",
    "                        for bz in batch_size:\n",
    "                            model = Sequential()\n",
    "                            model.add(Dense(10, input_dim = X_train.shape[1], activation = act1))\n",
    "                            model.add(Dense(15,activation = act2))\n",
    "                            model.add(Dense(1, activation = act3))\n",
    "                            model.compile(loss = 'binary_crossentropy', optimizer = SGD(lr = lrate), metrics = ['binary_accuracy'])\n",
    "                            history = model.fit(X_train, y_train, epochs=epoca, batch_size=bz, validation_split=0.2, verbose=0)\n",
    "                            precision = max(history.history.get('binary_accuracy'))\n",
    "                            perdida = max(history.history.get('loss'))\n",
    "                            dfaux.loc[i] = (epoca, act1, act2, act3, lrate, bz, precision, perdida)\n",
    "                            i = i+1\n",
    "        \n",
    "    return dfaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "epocas = [50, 100]\n",
    "act_capa1 = ['relu', 'sigmoid']\n",
    "act_capa2 = ['softmax', 'relu', 'sigmoid']\n",
    "act_capa3 = ['sigmoid', 'relu']\n",
    "learning_rate = [0.005]\n",
    "batch_size = [32, 64]\n",
    "DataFrame = modelo(epocas, act_capa1, act_capa2, act_capa3, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epocas</th>\n",
       "      <th>Capa1 - Funcion de activacion</th>\n",
       "      <th>Capa2 - Funcion de activacion</th>\n",
       "      <th>Capa3 - Funcion de activacion</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>precision</th>\n",
       "      <th>perdida</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.005</td>\n",
       "      <td>32</td>\n",
       "      <td>0.87125</td>\n",
       "      <td>2.134625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>100</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.005</td>\n",
       "      <td>32</td>\n",
       "      <td>0.86000</td>\n",
       "      <td>0.838125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.005</td>\n",
       "      <td>32</td>\n",
       "      <td>0.85875</td>\n",
       "      <td>4.038535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>100</td>\n",
       "      <td>relu</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.005</td>\n",
       "      <td>32</td>\n",
       "      <td>0.84250</td>\n",
       "      <td>2.767728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>100</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.005</td>\n",
       "      <td>64</td>\n",
       "      <td>0.82625</td>\n",
       "      <td>4.716605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epocas Capa1 - Funcion de activacion Capa2 - Funcion de activacion  \\\n",
       "6      50                          relu                          relu   \n",
       "28    100                          relu                          relu   \n",
       "30    100                          relu                          relu   \n",
       "34    100                          relu                       sigmoid   \n",
       "31    100                          relu                          relu   \n",
       "\n",
       "   Capa3 - Funcion de activacion  learning_rate batch_size  precision  \\\n",
       "6                           relu          0.005         32    0.87125   \n",
       "28                       sigmoid          0.005         32    0.86000   \n",
       "30                          relu          0.005         32    0.85875   \n",
       "34                          relu          0.005         32    0.84250   \n",
       "31                          relu          0.005         64    0.82625   \n",
       "\n",
       "     perdida  \n",
       "6   2.134625  \n",
       "28  0.838125  \n",
       "30  4.038535  \n",
       "34  2.767728  \n",
       "31  4.716605  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame.sort_values(by = ['precision', 'perdida'], ascending = [False, True]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6781937479972839\n",
      "0.800561785697937\n"
     ]
    }
   ],
   "source": [
    "max(history.history.get('loss')))\n",
    "print(max(history.history.get('binary_accuracy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim = X_train.shape[1], activation = 'relu'))\n",
    "model.add(Dense(15,activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'relu'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = SGD(lr = 0.005), metrics = ['binary_accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history[\"binary_accuracy\"])\n",
    "plt.plot(history.history[\"val_binary_accuracy\"])\n",
    "plt.title(\"Accuracy progression\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.770888</td>\n",
       "      <td>-0.244376</td>\n",
       "      <td>-0.546555</td>\n",
       "      <td>0.249301</td>\n",
       "      <td>0.107639</td>\n",
       "      <td>-0.036476</td>\n",
       "      <td>-0.857477</td>\n",
       "      <td>1.630853</td>\n",
       "      <td>-0.681858</td>\n",
       "      <td>-0.885102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.241953</td>\n",
       "      <td>-0.230902</td>\n",
       "      <td>-0.544276</td>\n",
       "      <td>-0.488085</td>\n",
       "      <td>0.126029</td>\n",
       "      <td>-0.664505</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>0.012705</td>\n",
       "      <td>0.295043</td>\n",
       "      <td>1.132725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.382299</td>\n",
       "      <td>0.527408</td>\n",
       "      <td>0.083051</td>\n",
       "      <td>0.756514</td>\n",
       "      <td>-0.076691</td>\n",
       "      <td>2.552384</td>\n",
       "      <td>1.532910</td>\n",
       "      <td>0.767078</td>\n",
       "      <td>-0.779461</td>\n",
       "      <td>-1.231827</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.947334</td>\n",
       "      <td>0.565024</td>\n",
       "      <td>-0.782613</td>\n",
       "      <td>-1.603208</td>\n",
       "      <td>0.327914</td>\n",
       "      <td>-2.898076</td>\n",
       "      <td>0.087941</td>\n",
       "      <td>-1.468756</td>\n",
       "      <td>0.472766</td>\n",
       "      <td>-0.753722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.099245</td>\n",
       "      <td>0.148788</td>\n",
       "      <td>0.383718</td>\n",
       "      <td>-1.197972</td>\n",
       "      <td>0.096624</td>\n",
       "      <td>-0.264744</td>\n",
       "      <td>-0.887411</td>\n",
       "      <td>0.503872</td>\n",
       "      <td>1.607185</td>\n",
       "      <td>0.239326</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.748431</td>\n",
       "      <td>-0.012384</td>\n",
       "      <td>0.882850</td>\n",
       "      <td>0.460390</td>\n",
       "      <td>-1.446728</td>\n",
       "      <td>0.810084</td>\n",
       "      <td>-0.961888</td>\n",
       "      <td>1.267785</td>\n",
       "      <td>-0.376854</td>\n",
       "      <td>0.613418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.690914</td>\n",
       "      <td>0.491432</td>\n",
       "      <td>1.881424</td>\n",
       "      <td>0.208917</td>\n",
       "      <td>-1.158740</td>\n",
       "      <td>-0.109687</td>\n",
       "      <td>1.738842</td>\n",
       "      <td>-0.094442</td>\n",
       "      <td>-1.538588</td>\n",
       "      <td>-1.074213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.984182</td>\n",
       "      <td>2.371374</td>\n",
       "      <td>0.156955</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>1.340171</td>\n",
       "      <td>1.247455</td>\n",
       "      <td>-0.920811</td>\n",
       "      <td>-0.123204</td>\n",
       "      <td>-0.013894</td>\n",
       "      <td>-1.732608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.335876</td>\n",
       "      <td>0.709564</td>\n",
       "      <td>-0.477003</td>\n",
       "      <td>-1.777018</td>\n",
       "      <td>-0.859206</td>\n",
       "      <td>1.069894</td>\n",
       "      <td>2.128010</td>\n",
       "      <td>-1.486141</td>\n",
       "      <td>-0.775667</td>\n",
       "      <td>0.212831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278521</td>\n",
       "      <td>-2.046240</td>\n",
       "      <td>-0.770221</td>\n",
       "      <td>0.351294</td>\n",
       "      <td>0.457445</td>\n",
       "      <td>-0.207627</td>\n",
       "      <td>0.388619</td>\n",
       "      <td>-0.520072</td>\n",
       "      <td>1.762115</td>\n",
       "      <td>-0.248894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  2.770888 -0.244376 -0.546555  0.249301  0.107639 -0.036476 -0.857477   \n",
       "1 -0.382299  0.527408  0.083051  0.756514 -0.076691  2.552384  1.532910   \n",
       "2 -0.099245  0.148788  0.383718 -1.197972  0.096624 -0.264744 -0.887411   \n",
       "3 -0.690914  0.491432  1.881424  0.208917 -1.158740 -0.109687  1.738842   \n",
       "4  0.335876  0.709564 -0.477003 -1.777018 -0.859206  1.069894  2.128010   \n",
       "\n",
       "         7         8         9   ...        30        31        32        33  \\\n",
       "0  1.630853 -0.681858 -0.885102  ... -0.241953 -0.230902 -0.544276 -0.488085   \n",
       "1  0.767078 -0.779461 -1.231827  ... -0.947334  0.565024 -0.782613 -1.603208   \n",
       "2  0.503872  1.607185  0.239326  ... -0.748431 -0.012384  0.882850  0.460390   \n",
       "3 -0.094442 -1.538588 -1.074213  ...  0.984182  2.371374  0.156955  0.265300   \n",
       "4 -1.486141 -0.775667  0.212831  ...  0.278521 -2.046240 -0.770221  0.351294   \n",
       "\n",
       "         34        35        36        37        38        39  \n",
       "0  0.126029 -0.664505  0.030457  0.012705  0.295043  1.132725  \n",
       "1  0.327914 -2.898076  0.087941 -1.468756  0.472766 -0.753722  \n",
       "2 -1.446728  0.810084 -0.961888  1.267785 -0.376854  0.613418  \n",
       "3  1.340171  1.247455 -0.920811 -0.123204 -0.013894 -1.732608  \n",
       "4  0.457445 -0.207627  0.388619 -0.520072  1.762115 -0.248894  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = (model.predict(test) > 0.5).astype(\"int32\")\n",
    "l2 = []\n",
    "for i in range(len(output)):\n",
    "    l2.append(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame()\n",
    "df3['Id'] = l2\n",
    "df3['Solution'] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Solution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8995</td>\n",
       "      <td>8996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8996</td>\n",
       "      <td>8997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8997</td>\n",
       "      <td>8998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8998</td>\n",
       "      <td>8999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8999</td>\n",
       "      <td>9000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  Solution\n",
       "0        1         1\n",
       "1        2         0\n",
       "2        3         1\n",
       "3        4         0\n",
       "4        5         0\n",
       "...    ...       ...\n",
       "8995  8996         1\n",
       "8996  8997         1\n",
       "8997  8998         1\n",
       "8998  8999         0\n",
       "8999  9000         1\n",
       "\n",
       "[9000 rows x 2 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv(r'M:/Users/angui/Desktop/proyectos_personales/datascienceLondon/predictions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
